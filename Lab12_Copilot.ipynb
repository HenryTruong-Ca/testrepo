{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely! Let's create functions for each of the tasks you've outlined. Each function will handle a specific step in your data analysis process. This modular approach makes your code organized and reusable.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Install and Import Libraries**\n",
    "\n",
    "```python\n",
    "def import_libraries():\n",
    "    \"\"\"\n",
    "    Imports necessary libraries for data manipulation and visualization.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    # Set visualization styles\n",
    "    sns.set(style='whitegrid')\n",
    "    plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "    print(\"Libraries imported successfully.\")\n",
    "    return pd, np, plt, sns\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This function imports the essential libraries:\n",
    "\n",
    "- **pandas** for data manipulation.\n",
    "- **numpy** for numerical operations.\n",
    "- **matplotlib.pyplot** and **seaborn** for data visualization.\n",
    "\n",
    "We also set some default styles for better-looking plots.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Load and Preview the Dataset**\n",
    "\n",
    "```python\n",
    "def load_and_preview_dataset(url):\n",
    "    \"\"\"\n",
    "    Loads the dataset from the provided URL and displays the first few rows.\n",
    "    Parameters:\n",
    "    - url (str): The URL to load the dataset from.\n",
    "    Returns:\n",
    "    - df (DataFrame): The loaded DataFrame.\n",
    "    \"\"\"\n",
    "    pd, _, _, _ = import_libraries()\n",
    "    try:\n",
    "        df = pd.read_csv(url)\n",
    "        print(\"Dataset loaded successfully.\")\n",
    "        print(\"First five rows of the dataset:\")\n",
    "        print(df.head())\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while loading the dataset:\", e)\n",
    "        return None\n",
    "```\n",
    "\n",
    "**Usage Example:**\n",
    "\n",
    "```python\n",
    "dataset_url = 'your_dataset_url.csv'\n",
    "df = load_and_preview_dataset(dataset_url)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The function loads data from a given URL.\n",
    "- It displays the first few rows to give you an overview.\n",
    "- Error handling is included to catch any issues during loading.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Handling Missing Data**\n",
    "\n",
    "```python\n",
    "def handle_missing_data(df, critical_columns):\n",
    "    \"\"\"\n",
    "    Identifies and manages missing values in critical columns.\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame to process.\n",
    "    - critical_columns (list): List of critical columns to check.\n",
    "    Returns:\n",
    "    - df (DataFrame): The DataFrame after handling missing data.\n",
    "    \"\"\"\n",
    "    pd, _, _, _ = import_libraries()\n",
    "    print(\"Handling missing data...\")\n",
    "\n",
    "    # Check initial shape\n",
    "    initial_shape = df.shape\n",
    "    print(f\"Initial dataset shape: {initial_shape}\")\n",
    "\n",
    "    # Drop rows with missing values in critical columns\n",
    "    df = df.dropna(subset=critical_columns)\n",
    "\n",
    "    # Check final shape\n",
    "    final_shape = df.shape\n",
    "    print(f\"Final dataset shape after dropping missing values: {final_shape}\")\n",
    "    print(f\"Total rows dropped: {initial_shape[0] - final_shape[0]}\")\n",
    "\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Usage Example:**\n",
    "\n",
    "```python\n",
    "critical_cols = ['Employment', 'JobSat', 'RemoteWork']\n",
    "df = handle_missing_data(df, critical_cols)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Drops rows where any of the critical columns have missing values.\n",
    "- Prints out the number of rows dropped for transparency.\n",
    "- This approach ensures that analyses on these columns are based on complete data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Analysis of Experience and Job Satisfaction**\n",
    "\n",
    "```python\n",
    "def analyze_experience_job_satisfaction(df):\n",
    "    \"\"\"\n",
    "    Analyzes the relationship between YearsCodePro and JobSat.\n",
    "    Returns:\n",
    "    - experience_median_job_sat (DataFrame): Median JobSat for each experience range.\n",
    "    \"\"\"\n",
    "    pd, plt, _, sns = import_libraries()\n",
    "    print(\"Analyzing experience and job satisfaction...\")\n",
    "\n",
    "    # Ensure 'YearsCodePro' is numeric\n",
    "    df = df.copy()\n",
    "    df['YearsCodePro'] = pd.to_numeric(df['YearsCodePro'], errors='coerce')\n",
    "\n",
    "    # Define experience ranges\n",
    "    max_experience = df['YearsCodePro'].max()\n",
    "    bins = [0, 5, 10, 20, max_experience]\n",
    "    labels = ['0-5', '5-10', '10-20', '>20']\n",
    "    df['ExperienceRange'] = pd.cut(df['YearsCodePro'], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "\n",
    "    # Calculate median JobSat for each range\n",
    "    experience_median_job_sat = df.groupby('ExperienceRange')['JobSat'].median().reset_index()\n",
    "\n",
    "    print(\"Median Job Satisfaction by Experience Range:\")\n",
    "    print(experience_median_job_sat)\n",
    "\n",
    "    # Visualize the relationship\n",
    "    sns.barplot(x='ExperienceRange', y='JobSat', data=df, ci=None, estimator=np.median, palette='viridis')\n",
    "    plt.title('Median Job Satisfaction by Experience Range')\n",
    "    plt.xlabel('Years of Professional Coding Experience')\n",
    "    plt.ylabel('Median Job Satisfaction')\n",
    "    plt.show()\n",
    "\n",
    "    return experience_median_job_sat\n",
    "```\n",
    "\n",
    "**Usage Example:**\n",
    "\n",
    "```python\n",
    "experience_job_sat = analyze_experience_job_satisfaction(df)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Converts `YearsCodePro` to numeric and handles non-numeric entries.\n",
    "- Categorizes experience into defined ranges.\n",
    "- Calculates and prints the median job satisfaction for each range.\n",
    "- Visualizes the results for better understanding.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Visualize Job Satisfaction**\n",
    "\n",
    "```python\n",
    "def visualize_job_satisfaction(df):\n",
    "    \"\"\"\n",
    "    Creates a count plot to show the distribution of JobSat values.\n",
    "    \"\"\"\n",
    "    pd, plt, _, sns = import_libraries()\n",
    "    print(\"Visualizing job satisfaction...\")\n",
    "\n",
    "    # Plot the distribution\n",
    "    sns.countplot(x='JobSat', data=df, palette='coolwarm', order=sorted(df['JobSat'].unique()))\n",
    "    plt.title('Distribution of Job Satisfaction')\n",
    "    plt.xlabel('Job Satisfaction')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "**Usage Example:**\n",
    "\n",
    "```python\n",
    "visualize_job_satisfaction(df)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Provides a visual representation of how job satisfaction is distributed among respondents.\n",
    "- Uses sorting to present the satisfaction levels in order.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Analyzing Remote Work Preferences by Job Role**\n",
    "\n",
    "```python\n",
    "def analyze_remote_work_preferences(df):\n",
    "    \"\"\"\n",
    "    Analyzes trends in remote work based on job roles.\n",
    "    \"\"\"\n",
    "    pd, plt, _, sns = import_libraries()\n",
    "    print(\"Analyzing remote work preferences by job role...\")\n",
    "\n",
    "    # Plot remote work distribution\n",
    "    sns.countplot(x='RemoteWork', data=df, palette='Set2', order=sorted(df['RemoteWork'].unique()))\n",
    "    plt.title('Distribution of Remote Work Preferences')\n",
    "    plt.xlabel('Remote Work')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Cross-tabulate remote work by employment type\n",
    "    remote_employment_ct = pd.crosstab(df['Employment'], df['RemoteWork'])\n",
    "    print(\"Remote Work Preferences by Employment Type:\")\n",
    "    print(remote_employment_ct)\n",
    "\n",
    "    # Heatmap of the cross-tabulation\n",
    "    sns.heatmap(remote_employment_ct, annot=True, fmt='d', cmap='YlGnBu')\n",
    "    plt.title('Remote Work Preferences by Employment Type')\n",
    "    plt.ylabel('Employment Type')\n",
    "    plt.xlabel('Remote Work Preference')\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "**Usage Example:**\n",
    "\n",
    "```python\n",
    "analyze_remote_work_preferences(df)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Visualizes how remote work preferences vary across the dataset.\n",
    "- Provides a heatmap to show the relationship between employment type and remote work preference.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 7: Analyzing Programming Language Trends by Region**\n",
    "\n",
    "```python\n",
    "def analyze_programming_language_trends(df, region_column='Country'):\n",
    "    \"\"\"\n",
    "    Analyzes the popularity of programming languages by region.\n",
    "    Parameters:\n",
    "    - df (DataFrame): The dataset to analyze.\n",
    "    - region_column (str): The column representing regions (e.g., 'Country').\n",
    "    \"\"\"\n",
    "    pd, plt, _, sns = import_libraries()\n",
    "    print(\"Analyzing programming language trends by region...\")\n",
    "\n",
    "    # Split the languages into lists\n",
    "    df['LanguagesWorkedWith'] = df['LanguageHaveWorkedWith'].str.split(';')\n",
    "\n",
    "    # Explode the languages\n",
    "    df_exploded = df.explode('LanguagesWorkedWith')\n",
    "\n",
    "    # Group and count\n",
    "    language_region_counts = df_exploded.groupby([region_column, 'LanguagesWorkedWith']).size().reset_index(name='Counts')\n",
    "\n",
    "    # For illustration, let's focus on the top 5 regions\n",
    "    top_regions = df[region_column].value_counts().head(5).index\n",
    "    filtered_data = language_region_counts[language_region_counts[region_column].isin(top_regions)]\n",
    "\n",
    "    # Pivot for heatmap\n",
    "    pivot_table = filtered_data.pivot(index='LanguagesWorkedWith', columns=region_column, values='Counts').fillna(0)\n",
    "\n",
    "    # Visualize with a heatmap\n",
    "    sns.heatmap(pivot_table, cmap='Blues')\n",
    "    plt.title('Programming Language Popularity by Region')\n",
    "    plt.ylabel('Programming Language')\n",
    "    plt.xlabel('Region')\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "**Usage Example:**\n",
    "\n",
    "```python\n",
    "analyze_programming_language_trends(df)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Handles multiple programming languages per respondent by splitting and exploding the data.\n",
    "- Focuses analysis on the top regions for clarity.\n",
    "- Uses a heatmap to show which languages are most popular in different regions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 8: Correlation Between Experience and Satisfaction**\n",
    "\n",
    "```python\n",
    "def correlate_experience_satisfaction(df):\n",
    "    \"\"\"\n",
    "    Examines how YearsCodePro correlates with JobSatPoints_1.\n",
    "    \"\"\"\n",
    "    pd, plt, _, sns = import_libraries()\n",
    "    print(\"Analyzing correlation between experience and satisfaction...\")\n",
    "\n",
    "    # Ensure numeric types\n",
    "    df['YearsCodePro'] = pd.to_numeric(df['YearsCodePro'], errors='coerce')\n",
    "    df['JobSatPoints_1'] = pd.to_numeric(df['JobSatPoints_1'], errors='coerce')\n",
    "\n",
    "    # Drop missing values\n",
    "    df_clean = df.dropna(subset=['YearsCodePro', 'JobSatPoints_1'])\n",
    "\n",
    "    # Scatter plot\n",
    "    sns.scatterplot(x='YearsCodePro', y='JobSatPoints_1', data=df_clean)\n",
    "    plt.title('Correlation Between Experience and Job Satisfaction')\n",
    "    plt.xlabel('Years of Professional Coding Experience')\n",
    "    plt.ylabel('Job Satisfaction Points')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate correlation\n",
    "    correlation = df_clean['YearsCodePro'].corr(df_clean['JobSatPoints_1'])\n",
    "    print(f\"Pearson correlation coefficient: {correlation:.2f}\")\n",
    "```\n",
    "\n",
    "**Usage Example:**\n",
    "\n",
    "```python\n",
    "correlate_experience_satisfaction(df)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Converts relevant columns to numeric types.\n",
    "- Visualizes the relationship with a scatter plot.\n",
    "- Calculates and prints the Pearson correlation coefficient.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 9: Educational Background and Employment Type**\n",
    "\n",
    "```python\n",
    "def analyze_education_employment(df):\n",
    "    \"\"\"\n",
    "    Explores how educational background relates to employment type.\n",
    "    \"\"\"\n",
    "    pd, plt, _, sns = import_libraries()\n",
    "    print(\"Analyzing educational background and employment type...\")\n",
    "\n",
    "    # Cross-tabulation\n",
    "    education_employment_ct = pd.crosstab(df['EdLevel'], df['Employment'])\n",
    "\n",
    "    print(\"Educational Background vs. Employment Type:\")\n",
    "    print(education_employment_ct)\n",
    "\n",
    "    # Visualize with a heatmap\n",
    "    sns.heatmap(education_employment_ct, annot=True, fmt='d', cmap='coolwarm')\n",
    "    plt.title('Education Level by Employment Type')\n",
    "    plt.ylabel('Education Level')\n",
    "    plt.xlabel('Employment Type')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "**Usage Example:**\n",
    "\n",
    "```python\n",
    "analyze_education_employment(df)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Uses cross-tabulation to examine the relationship.\n",
    "- Provides a heatmap for visual insight.\n",
    "- Helps understand if higher education levels correlate with certain employment types.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 10: Save the Cleaned and Analyzed Dataset**\n",
    "\n",
    "```python\n",
    "def save_cleaned_dataset(df, filename='cleaned_dataset.csv'):\n",
    "    \"\"\"\n",
    "    Saves the modified dataset to a CSV file.\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame to save.\n",
    "    - filename (str): The name of the file to save to.\n",
    "    \"\"\"\n",
    "    pd, _, _, _ = import_libraries()\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Dataset saved successfully as {filename}.\")\n",
    "```\n",
    "\n",
    "**Usage Example:**\n",
    "\n",
    "```python\n",
    "save_cleaned_dataset(df)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Saves the DataFrame to a CSV file without the index.\n",
    "- Default filename is 'cleaned_dataset.csv', but you can specify any name.\n",
    "\n",
    "---\n",
    "\n",
    "### **Putting It All Together**\n",
    "\n",
    "Here's how you might use these functions in your script:\n",
    "\n",
    "```python\n",
    "# Import libraries\n",
    "pd, np, plt, sns = import_libraries()\n",
    "\n",
    "# Step 2: Load and preview the dataset\n",
    "dataset_url = 'your_dataset_url.csv'  # Replace with your actual dataset URL or file path\n",
    "df = load_and_preview_dataset(dataset_url)\n",
    "\n",
    "# Step 3: Handle missing data\n",
    "critical_columns = ['Employment', 'JobSat', 'RemoteWork']\n",
    "df = handle_missing_data(df, critical_columns)\n",
    "\n",
    "# Step 4: Analyze experience and job satisfaction\n",
    "analyze_experience_job_satisfaction(df)\n",
    "\n",
    "# Step 5: Visualize job satisfaction\n",
    "visualize_job_satisfaction(df)\n",
    "\n",
    "# Step 6: Analyze remote work preferences\n",
    "analyze_remote_work_preferences(df)\n",
    "\n",
    "# Step 7: Analyze programming language trends\n",
    "analyze_programming_language_trends(df)\n",
    "\n",
    "# Step 8: Correlation between experience and satisfaction\n",
    "correlate_experience_satisfaction(df)\n",
    "\n",
    "# Step 9: Analyze educational background and employment type\n",
    "analyze_education_employment(df)\n",
    "\n",
    "# Step 10: Save the cleaned and analyzed dataset\n",
    "save_cleaned_dataset(df, 'analyzed_dataset.csv')\n",
    "```\n",
    "\n",
    "**Note:** Ensure that your dataset contains the columns used in these functions. If column names differ, adjust the code accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "### **Additional Considerations**\n",
    "\n",
    "- **Data Privacy:** Always ensure you have the right to use and share the dataset, respecting privacy and data protection regulations.\n",
    "- **Error Handling:** Implement additional error checks as needed, especially when working with real data.\n",
    "- **Customization:** Feel free to enhance these functions to suit your specific analysis needs.\n",
    "- **Visualization Themes:** Customize plots with your preferred color palettes and styles for better presentation.\n",
    "\n",
    "---\n",
    "\n",
    "By modularizing your code into functions, you not only make it cleaner but also reusable for future projects. This approach lays a solid foundation for efficient data analysis pipelines.\n",
    "\n",
    "Is there anything else you'd like to explore or any specific area you'd like to delve deeper into?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
