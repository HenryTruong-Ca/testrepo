{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely! Let's tackle each of these tasks by creating Python functions that will help you preprocess and analyze your dataset effectively. I'll provide detailed explanations for each function to ensure you understand how they work and why they're essential.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Load the Dataset**\n",
    "\n",
    "### **1.1 Import Necessary Libraries and Load the Dataset**\n",
    "\n",
    "Let's start by creating a function to import the necessary libraries and load your dataset into a pandas DataFrame.\n",
    "\n",
    "```python\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Imports required libraries and loads the dataset into a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the CSV file containing the dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The loaded DataFrame.\n",
    "    \"\"\"\n",
    "    # Import necessary libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Load the dataset\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Dataset loaded successfully with shape {df.shape}.\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"The file was not found. Please check the file path.\")\n",
    "        return None\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Imports:**\n",
    "  - `pandas` for data manipulation.\n",
    "  - `numpy` for numerical computations.\n",
    "- **Functionality:**\n",
    "  - Tries to read the CSV file located at `file_path` using `pd.read_csv()`.\n",
    "  - If successful, prints the shape of the DataFrame and returns it.\n",
    "  - If the file is not found, prints an error message.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Explore the Dataset**\n",
    "\n",
    "### **2.1 Summarize the Dataset by Displaying Column Data Types, Counts, and Missing Values**\n",
    "\n",
    "```python\n",
    "def summarize_dataset(df):\n",
    "    \"\"\"\n",
    "    Summarizes the dataset by displaying column data types, counts, and missing values.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to summarize.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"Dataset Summary:\")\n",
    "    print(\"\\nColumn Data Types:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\nColumn Counts and Missing Values:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(pd.DataFrame({'Count': df.count(), 'Missing Values': missing_values}))\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Functionality:**\n",
    "  - Prints data types of each column using `df.dtypes`.\n",
    "  - Calculates and displays the count of non-missing values and the number of missing values per column.\n",
    "\n",
    "### **2.2 Generate Basic Statistics for Numerical Columns**\n",
    "\n",
    "```python\n",
    "def numerical_statistics(df):\n",
    "    \"\"\"\n",
    "    Generates basic statistics for numerical columns in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing numerical columns.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"\\nNumerical Columns Statistics:\")\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    print(df[numerical_cols].describe())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Functionality:**\n",
    "  - Selects numerical columns using `select_dtypes(include=[np.number])`.\n",
    "  - Uses `describe()` to display statistics like mean, median, standard deviation, min, and max.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Identifying and Removing Inconsistencies**\n",
    "\n",
    "### **3.1 Identify Inconsistent or Irrelevant Entries in Specific Columns (e.g., Country)**\n",
    "\n",
    "```python\n",
    "def identify_inconsistencies(df, column_name):\n",
    "    \"\"\"\n",
    "    Identifies inconsistent or irrelevant entries in a specific column.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to analyze.\n",
    "        column_name (str): The column to check for inconsistencies.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Counts of unique values in the column.\n",
    "    \"\"\"\n",
    "    value_counts = df[column_name].value_counts(dropna=False)\n",
    "    print(f\"\\nUnique values in '{column_name}':\")\n",
    "    print(value_counts)\n",
    "    return value_counts\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Functionality:**\n",
    "  - Counts occurrences of each unique value in the specified column, including missing values.\n",
    "  - Allows you to inspect and identify inconsistencies.\n",
    "\n",
    "### **3.2 Standardize Entries in Columns Like Country or EdLevel**\n",
    "\n",
    "```python\n",
    "def standardize_entries(df, column_name, mapping_dict):\n",
    "    \"\"\"\n",
    "    Standardizes entries in a column by mapping inconsistent values to a consistent format.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to modify.\n",
    "        column_name (str): The column to standardize.\n",
    "        mapping_dict (dict): A dictionary mapping old values to new standardized values.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with standardized column entries.\n",
    "    \"\"\"\n",
    "    df[column_name] = df[column_name].replace(mapping_dict)\n",
    "    print(f\"\\nStandardized '{column_name}' column.\")\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Functionality:**\n",
    "  - Replaces inconsistent values in the specified column using a provided mapping dictionary.\n",
    "  - Updates the DataFrame with standardized entries.\n",
    "\n",
    "**Example Usage:**\n",
    "\n",
    "```python\n",
    "# Define a mapping dictionary for countries\n",
    "country_mapping = {\n",
    "    'United States of America': 'USA',\n",
    "    'United States': 'USA',\n",
    "    'U.S.': 'USA',\n",
    "    'UK': 'United Kingdom',\n",
    "    'England': 'United Kingdom'\n",
    "}\n",
    "df = standardize_entries(df, 'Country', country_mapping)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Encoding Categorical Variables**\n",
    "\n",
    "### **4.1 Encode the Employment Column Using One-Hot Encoding**\n",
    "\n",
    "```python\n",
    "def encode_categorical_one_hot(df, column_name):\n",
    "    \"\"\"\n",
    "    Encodes a categorical column using one-hot encoding.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to modify.\n",
    "        column_name (str): The column to encode.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with one-hot encoded columns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    one_hot = pd.get_dummies(df[column_name], prefix=column_name)\n",
    "    df = df.drop(column_name, axis=1)\n",
    "    df = df.join(one_hot)\n",
    "    print(f\"\\nOne-hot encoded '{column_name}' column.\")\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Functionality:**\n",
    "  - Uses `pd.get_dummies()` to perform one-hot encoding on the specified column.\n",
    "  - Drops the original column and joins the new one-hot encoded columns to the DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Handling Missing Values**\n",
    "\n",
    "### **5.1 Identify Columns with the Highest Number of Missing Values**\n",
    "\n",
    "```python\n",
    "def identify_missing_values(df):\n",
    "    \"\"\"\n",
    "    Identifies columns with the highest number of missing values.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing columns and their missing value counts and percentages.\n",
    "    \"\"\"\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_percent = (missing_counts / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_counts,\n",
    "        'Missing Percentage': missing_percent\n",
    "    }).sort_values(by='Missing Count', ascending=False)\n",
    "    print(\"\\nColumns with Missing Values:\")\n",
    "    print(missing_df[missing_df['Missing Count'] > 0])\n",
    "    return missing_df\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Functionality:**\n",
    "  - Calculates the number and percentage of missing values per column.\n",
    "  - Sorts and displays columns with missing values in descending order.\n",
    "\n",
    "### **5.2 Impute Missing Values in Numerical Columns with Mean or Median**\n",
    "\n",
    "```python\n",
    "def impute_numerical(df, column_name, strategy='mean'):\n",
    "    \"\"\"\n",
    "    Imputes missing values in a numerical column using mean or median.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to modify.\n",
    "        column_name (str): The numerical column to impute.\n",
    "        strategy (str): The imputation strategy ('mean' or 'median').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with imputed values in the specified column.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if strategy == 'mean':\n",
    "        impute_value = df[column_name].mean()\n",
    "    elif strategy == 'median':\n",
    "        impute_value = df[column_name].median()\n",
    "    else:\n",
    "        raise ValueError(\"Strategy must be 'mean' or 'median'.\")\n",
    "    df[column_name].fillna(impute_value, inplace=True)\n",
    "    print(f\"\\nImputed missing values in '{column_name}' with {strategy}: {impute_value}\")\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Functionality:**\n",
    "  - Calculates the mean or median of the specified numerical column.\n",
    "  - Fills missing values with the calculated statistic.\n",
    "\n",
    "### **5.3 Impute Missing Values in Categorical Columns with the Most Frequent Value**\n",
    "\n",
    "```python\n",
    "def impute_categorical(df, column_name):\n",
    "    \"\"\"\n",
    "    Imputes missing values in a categorical column with the most frequent value.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to modify.\n",
    "        column_name (str): The categorical column to impute.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with imputed values in the specified column.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    most_frequent = df[column_name].mode()[0]\n",
    "    df[column_name].fillna(most_frequent, inplace=True)\n",
    "    print(f\"\\nImputed missing values in '{column_name}' with the most frequent value: {most_frequent}\")\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Functionality:**\n",
    "  - Finds the most frequent value (mode) of the categorical column.\n",
    "  - Fills missing values with the mode.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Feature Scaling and Transformation**\n",
    "\n",
    "### **6.1 Apply Min-Max Scaling to Normalize the `ConvertedCompYearly` Column**\n",
    "\n",
    "```python\n",
    "def min_max_scale(df, column_name):\n",
    "    \"\"\"\n",
    "    Applies Min-Max Scaling to normalize a numerical column.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to modify.\n",
    "        column_name (str): The numerical column to scale.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with a new normalized column.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df[column_name + '_Normalized'] = scaler.fit_transform(df[[column_name]])\n",
    "    print(f\"\\nApplied Min-Max Scaling to '{column_name}'.\")\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Functionality:**\n",
    "  - Uses `MinMaxScaler` from `sklearn.preprocessing` to normalize the specified column.\n",
    "  - Adds a new column with the normalized values.\n",
    "\n",
    "### **6.2 Log-Transform the `ConvertedCompYearly` Column to Reduce Skewness**\n",
    "\n",
    "```python\n",
    "def log_transform(df, column_name):\n",
    "    \"\"\"\n",
    "    Applies log transformation to a numerical column to reduce skewness.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to modify.\n",
    "        column_name (str): The numerical column to transform.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with a new log-transformed column.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    import numpy as np\n",
    "\n",
    "    df[column_name + '_Log'] = np.log1p(df[column_name])\n",
    "    print(f\"\\nApplied log transformation to '{column_name}'.\")\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Functionality:**\n",
    "  - Uses `np.log1p()` for log transformation, which handles zero and positive values.\n",
    "  - Creates a new column with the log-transformed values.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Feature Engineering**\n",
    "\n",
    "### **7.1 Create a New Column `ExperienceLevel` Based on the `YearsCodePro` Column**\n",
    "\n",
    "```python\n",
    "def create_experience_level(df, column_name='YearsCodePro'):\n",
    "    \"\"\"\n",
    "    Creates a new column 'ExperienceLevel' based on the 'YearsCodePro' column.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to modify.\n",
    "        column_name (str): The column containing years of professional coding experience.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the new 'ExperienceLevel' column.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Ensure 'YearsCodePro' is numeric\n",
    "    df[column_name] = pd.to_numeric(df[column_name], errors='coerce')\n",
    "    \n",
    "    # Define experience level based on years of coding\n",
    "    def experience_level(years):\n",
    "        if pd.isnull(years):\n",
    "            return 'Unknown'\n",
    "        elif years < 1:\n",
    "            return 'Entry-level'\n",
    "        elif years < 3:\n",
    "            return 'Junior'\n",
    "        elif years < 5:\n",
    "            return 'Intermediate'\n",
    "        elif years < 10:\n",
    "            return 'Advanced'\n",
    "        else:\n",
    "            return 'Expert'\n",
    "    \n",
    "    df['ExperienceLevel'] = df[column_name].apply(experience_level)\n",
    "    print(f\"\\nCreated 'ExperienceLevel' column based on '{column_name}'.\")\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Functionality:**\n",
    "  - Converts the `YearsCodePro` column to numeric.\n",
    "  - Defines the `experience_level` function to categorize experience.\n",
    "  - Applies the function to create the `ExperienceLevel` column.\n",
    "\n",
    "---\n",
    "\n",
    "## **Putting It All Together**\n",
    "\n",
    "Here's how you can use these functions in your analysis pipeline:\n",
    "\n",
    "```python\n",
    "# Step 1: Load the Dataset\n",
    "file_path = 'your_dataset.csv'  # Replace with your actual file path\n",
    "df = load_dataset(file_path)\n",
    "\n",
    "if df is not None:\n",
    "    # Step 2: Explore the Dataset\n",
    "    summarize_dataset(df)\n",
    "    numerical_statistics(df)\n",
    "    \n",
    "    # Step 3: Identifying and Removing Inconsistencies\n",
    "    # Identify inconsistencies in 'Country'\n",
    "    country_values = identify_inconsistencies(df, 'Country')\n",
    "    # Standardize 'Country' entries\n",
    "    country_mapping = {'United States of America': 'USA', 'UK': 'United Kingdom'}  # Example mappings\n",
    "    df = standardize_entries(df, 'Country', country_mapping)\n",
    "    \n",
    "    # Step 4: Encoding Categorical Variables\n",
    "    df = encode_categorical_one_hot(df, 'Employment')\n",
    "    \n",
    "    # Step 5: Handling Missing Values\n",
    "    missing_df = identify_missing_values(df)\n",
    "    # Impute numerical column 'ConvertedCompYearly'\n",
    "    df = impute_numerical(df, 'ConvertedCompYearly', strategy='median')\n",
    "    # Impute categorical column 'RemoteWork'\n",
    "    df = impute_categorical(df, 'RemoteWork')\n",
    "    \n",
    "    # Step 6: Feature Scaling and Transformation\n",
    "    df = min_max_scale(df, 'ConvertedCompYearly')\n",
    "    df = log_transform(df, 'ConvertedCompYearly')\n",
    "    \n",
    "    # Step 7: Feature Engineering\n",
    "    df = create_experience_level(df)\n",
    "    \n",
    "    # Now, 'df' is ready for further analysis or modeling\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Additional Notes**\n",
    "\n",
    "- **Error Handling:** The functions include basic error handling and messages to guide you through any issues.\n",
    "- **Modularity:** Each function operates independently, allowing you to use only those that are relevant to your analysis.\n",
    "- **Customization:** Adjust the mappings, thresholds, and strategies within the functions to suit your specific dataset and requirements.\n",
    "- **Performance:** For large datasets, consider optimizing functions or processing data in chunks.\n",
    "\n",
    "---\n",
    "\n",
    "## **Visualizations and Additional Analysis**\n",
    "\n",
    "After preprocessing, you might want to visualize the data to gain insights:\n",
    "\n",
    "```python\n",
    "# Example: Visualize the distribution of 'ConvertedCompYearly' before and after log transformation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Original distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['ConvertedCompYearly'], bins=50, kde=True)\n",
    "plt.title('Original Compensation Distribution')\n",
    "plt.xlabel('ConvertedCompYearly')\n",
    "\n",
    "# Log-transformed distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df['ConvertedCompYearly_Log'], bins=50, kde=True, color='orange')\n",
    "plt.title('Log-Transformed Compensation Distribution')\n",
    "plt.xlabel('ConvertedCompYearly_Log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "By following these steps and utilizing the provided functions, you can effectively preprocess your dataset, handle missing values, encode categorical variables, scale numerical features, and engineer new features. This preparation is crucial for accurate and meaningful analysis, modeling, and interpretation of your data.\n",
    "\n",
    "Feel free to adjust the functions or ask for further assistance if you have specific needs or encounter any challenges during your data analysis journey!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
