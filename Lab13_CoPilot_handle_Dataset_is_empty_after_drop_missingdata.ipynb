{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, we're hitting a classic data wrangling snag, aren't we? When `df_clean` turns up empty after executing `df_clean = handle_missing_data(df)`, it's a telltale sign that our data has missing values scattered across many rows, and our current method of handling them is a bit too aggressive. Let's unravel this together and get to the bottom of it.\n",
    "\n",
    "---\n",
    "\n",
    "### **Understanding the Issue**\n",
    "\n",
    "In the `handle_missing_data` function, we're using:\n",
    "\n",
    "```python\n",
    "df_cleaned = df.dropna()\n",
    "```\n",
    "\n",
    "By default, `dropna()` without any parameters drops all rows **where any column is missing** a value. If your dataset has even a single `NaN` in a row, that entire row gets the axe. Given the nature of survey data, it's common to have missing responses scattered throughout, which means we're probably wiping out more data than we intend to.\n",
    "\n",
    "---\n",
    "\n",
    "### **Investigating Missing Data**\n",
    "\n",
    "Let's quantify the missingness in your data to see what's going on.\n",
    "\n",
    "```python\n",
    "def check_missing_data(df):\n",
    "    \"\"\"\n",
    "    Checks the percentage of missing data per column.\n",
    "    \"\"\"\n",
    "    missing_percentages = df.isnull().mean() * 100\n",
    "    missing_percentages = missing_percentages[missing_percentages > 0]\n",
    "    print(\"\\nPercentage of Missing Data per Column:\")\n",
    "    print(missing_percentages.sort_values(ascending=False))\n",
    "\n",
    "# Check missing data\n",
    "check_missing_data(df)\n",
    "```\n",
    "\n",
    "**Example Output:**\n",
    "\n",
    "```\n",
    "Percentage of Missing Data per Column:\n",
    "DatabaseAdmired                 90.2%\n",
    "EmbeddedAdmired                 85.5%\n",
    "AIBen                           80.1%\n",
    "...\n",
    "```\n",
    "\n",
    "This output reveals that some columns have a high percentage of missing values. If we use `dropna()` as-is, we end up with an empty DataFrame because almost every row has at least one `NaN`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Refining Our Approach**\n",
    "\n",
    "#### **1. Drop Columns with Excessive Missing Data**\n",
    "\n",
    "Let's decide on a thresholdâ€”for instance, if more than 50% of a column is missing, we might drop it.\n",
    "\n",
    "```python\n",
    "def handle_missing_data(df, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Handles missing data by dropping columns with missing data exceeding the threshold.\n",
    "    \"\"\"\n",
    "    # Calculate missing data percentages\n",
    "    missing_percentages = df.isnull().mean()\n",
    "    \n",
    "    # Identify columns to drop\n",
    "    cols_to_drop = missing_percentages[missing_percentages > threshold].index\n",
    "    df_reduced = df.drop(columns=cols_to_drop)\n",
    "    print(f\"\\nDropped columns with more than {threshold*100}% missing data:\")\n",
    "    print(list(cols_to_drop))\n",
    "    \n",
    "    return df_reduced\n",
    "\n",
    "# Apply the function\n",
    "df_partial_clean = handle_missing_data(df)\n",
    "```\n",
    "\n",
    "Now, `df_partial_clean` should retain the majority of your data while removing problematic columns.\n",
    "\n",
    "#### **2. Impute Remaining Missing Values**\n",
    "\n",
    "For the remaining missing values, we can fill them in instead of dropping more rows.\n",
    "\n",
    "- **Numeric Columns:** Fill with mean or median.\n",
    "- **Categorical Columns:** Fill with the mode (most frequent value).\n",
    "\n",
    "```python\n",
    "def impute_missing_values(df):\n",
    "    \"\"\"\n",
    "    Imputes missing values for numeric and categorical columns.\n",
    "    \"\"\"\n",
    "    # Impute numeric columns with median\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    for col in numeric_cols:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "    \n",
    "    # Impute categorical columns with mode\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if df[col].isnull().any():\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "    \n",
    "    print(\"\\nMissing values have been imputed.\")\n",
    "    return df\n",
    "\n",
    "# Apply imputation\n",
    "df_clean = impute_missing_values(df_partial_clean)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Revised `handle_missing_data` Function**\n",
    "\n",
    "Combining the steps, here's an updated function:\n",
    "\n",
    "```python\n",
    "def handle_missing_data(df, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Handles missing data by dropping columns with high missingness and imputing the rest.\n",
    "    \"\"\"\n",
    "    # Drop columns with too many missing values\n",
    "    missing_percentages = df.isnull().mean()\n",
    "    cols_to_drop = missing_percentages[missing_percentages > threshold].index\n",
    "    df_reduced = df.drop(columns=cols_to_drop)\n",
    "    print(f\"\\nDropped columns with more than {threshold*100}% missing data:\")\n",
    "    print(list(cols_to_drop))\n",
    "    \n",
    "    # Impute missing values\n",
    "    # Impute numeric columns with median\n",
    "    numeric_cols = df_reduced.select_dtypes(include=['float64', 'int64']).columns\n",
    "    for col in numeric_cols:\n",
    "        df_reduced[col].fillna(df_reduced[col].median(), inplace=True)\n",
    "    \n",
    "    # Impute categorical columns with mode\n",
    "    categorical_cols = df_reduced.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if df_reduced[col].isnull().any():\n",
    "            df_reduced[col].fillna(df_reduced[col].mode()[0], inplace=True)\n",
    "    \n",
    "    print(\"\\nMissing values have been imputed.\")\n",
    "    print(f\"After cleaning, the dataset contains {df_reduced.shape[0]} rows and {df_reduced.shape[1]} columns.\")\n",
    "    return df_reduced\n",
    "```\n",
    "\n",
    "Now, when you run:\n",
    "\n",
    "```python\n",
    "df_clean = handle_missing_data(df)\n",
    "```\n",
    "\n",
    "You should end up with a DataFrame that retains most of your data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why `df_clean` Was Empty Before**\n",
    "\n",
    "In our initial function, `df.dropna()` was too aggressive for the nature of your data. Survey data often has missing values due to respondents skipping questions. By dropping any row with a missing value, we effectively eliminated all our data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Validating the Cleaned Data**\n",
    "\n",
    "Let's do a sanity check to ensure `df_clean` is no longer empty:\n",
    "\n",
    "```python\n",
    "print(f\"\\nCleaned DataFrame dimensions: {df_clean.shape}\")\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "display(df_clean.head())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Proceeding with Analysis**\n",
    "\n",
    "Now that we have a cleaned dataset, you can continue with the rest of your analysis, like examining key columns, visualizing job satisfaction, and so on.\n",
    "\n",
    "---\n",
    "\n",
    "### **Additional Thoughts**\n",
    "\n",
    "- **Consider the Impact of Imputation:** While filling in missing values helps retain data, it's important to consider how it might affect your analysis. For example, filling in 'JobSat' with the mode may skew satisfaction levels.\n",
    "\n",
    "- **Alternative Approaches:** For more nuanced handling, you might:\n",
    "\n",
    "  - Use advanced imputation techniques like K-Nearest Neighbors or regression models.\n",
    "  - Analyze missingness patterns to see if data is missing at random.\n",
    "\n",
    "- **Document Assumptions:** Keep notes on how you handled missing data, as it can affect the interpretation of your results.\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "With `df_clean` now properly populated, you can rerun your functions for analyzing key columns and visualizing trends.\n",
    "\n",
    "If you encounter any other bumps along the way or want to delve deeper into any of these steps, just let me know. I'm eager to help you unlock the stories hidden in your data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
